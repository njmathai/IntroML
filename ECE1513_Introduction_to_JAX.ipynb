{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ECE1513_Introduction_to_JAX",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/njmathai/IntroML/blob/master/ECE1513_Introduction_to_JAX.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Vw6FR1FUfD1"
      },
      "source": [
        "# 1.0 Introduction to JAX\n",
        "Main references:\n",
        "- [1] https://github.com/google/jax\n",
        "\n",
        "- [2] https://colindcarroll.com/2019/04/06/exercises-in-automatic-differentiation-using-autograd-and-jax/\n",
        "\n",
        "- [3] https://github.com/HIPS/autograd/blob/master/docs/tutorial.md\n",
        "- [4] https://colinraffel.com/blog/you-don-t-know-jax.html\n",
        "- [5] https://jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html\n",
        "\n",
        "\n",
        "*At its core, JAX is an extensible system for transforming numerical functions.* JAX implements an updated version of **autograd**. Autograd's `grad` function takes in a **scalar** function, and returns to you the gradient function. \n",
        "\n",
        " In this tutorial we will go through some of the main features included in the JAX package: ```grad``` and ```jit```.\n",
        "\n",
        " One of the main features you will probably use during this course is the ```grad``` function for computing gradients of your loss functions with respect to your model's weights.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnUOTQXHqMfv"
      },
      "source": [
        "## 1.1 Step-by-step example on how to use ```jax.grad``` for auto-differentiation.\n",
        "\n",
        "**1) Define your function**\n",
        "\n",
        "$$y = \\sigma(wx + b),\\ \\sigma(x)=\\frac{1}{1+e^{-x}}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4l38DEGQUitN"
      },
      "source": [
        "import jax.numpy as np\n",
        "from jax import grad, jit, vmap, random, value_and_grad\n",
        "import itertools\n",
        "import time\n",
        "\n",
        "\n",
        "key = random.PRNGKey(1)\n",
        "\n",
        "x = random.normal(key)\n",
        "w = random.normal(key+1)\n",
        "b = random.normal(key+2)\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1/(1 + np.exp(-x))\n",
        "\n",
        "# \n",
        "def make_linear_sigmoid(x): \n",
        "  def predict(W, b): # Here, you define the function with the parameters that define your model\n",
        "    return sigmoid(np.dot(x, W) + b)\n",
        "  return predict "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6F1D2CpUvDK"
      },
      "source": [
        "## **2) Define your gradient with respect to the fitting weights**\n",
        "\n",
        "\n",
        "The function `grad` takes as arguments:\n",
        "-  `fun`: the numpy **function** for which the computation of the gradients is needed.\n",
        "- `argnums`: the **arguments** of the functions  with respect to which the function will be differentiated\n",
        "\n",
        "```\n",
        "grad_not_jit(fun = make_linear_sigmoid(x),\n",
        "                              argnums =  (0,1))\n",
        "\n",
        "```\n",
        "Returns the evaluated **gradients**.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "**Quick quiz**: what would you change in the code above if you needed to differentiate the function only with respect to $b$?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSKXLHE0U2p2"
      },
      "source": [
        "**Using jit to speed up functions (VERY IMPORTANT FOR THE FUTURE)**\n",
        "\n",
        "JAX provides jit (just in time compiler) which takes Python (using numpy) functions and compiles them such that they can be run efficiently on the chosen accelerators (CPU/GPU/TPU). Using jit could significantly speed up your computations and it requires little to no overhead in your coding. Let's have a look at a simple example. It is sufficient to use the jit decorator in front of your function such that the declared operation are compiled in advance (only once) and the code will run much more efficiently without any interpreter overhead."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RlNiA1psU4u6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "outputId": "59afbe25-7b13-4a5b-bf1e-3164cea26a8f"
      },
      "source": [
        "# I am ignoring the values returned for timing because of issues with the scoping of functions (timeit issue)\n",
        "grad_not_jit = grad(make_linear_sigmoid(x), (0,1))\n",
        "%timeit _, _ = grad_not_jit(w,b) # AUTOGRAD\n",
        "\n",
        "grad_jit = jit(grad(make_linear_sigmoid(x), (0,1)))\n",
        "%timeit _, _ = grad_jit(w,b) # AUTOGRAD\n",
        "\n",
        "# Check with the analytical computation\n",
        "w_gradient , b_gradient = grad_jit(w,b) # AUTOGRAD\n",
        "\n",
        "w_gradient_manual = sigmoid(np.dot(x, w) + b) * (1 - sigmoid(np.dot(x, w) + b)) * x\n",
        "b_gradient_manual = sigmoid(np.dot(x, w) + b) * (1 - sigmoid(np.dot(x, w) + b)) \n",
        "\n",
        "print()\n",
        "print('Autograd result : w')\n",
        "print(w_gradient)\n",
        "print('Manually derived result : w')\n",
        "print(w_gradient_manual)\n",
        "\n",
        "print('Autograd result : b')\n",
        "print(b_gradient)\n",
        "print('Manually derived result : b')\n",
        "print(b_gradient_manual)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The slowest run took 126.74 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
            "1 loop, best of 3: 3.82 ms per loop\n",
            "The slowest run took 725.32 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
            "10000 loops, best of 3: 153 µs per loop\n",
            "\n",
            "Autograd result : w\n",
            "-0.26804116\n",
            "Manually derived result : w\n",
            "-0.2680411\n",
            "Autograd result : b\n",
            "0.22633177\n",
            "Manually derived result : b\n",
            "0.22633173\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zafRRNriqeyq"
      },
      "source": [
        "#2.0 Review of Linear Regression\n",
        "As studied in your lecture, in linear regression, we are given a data set $\\mathcal{D} =  \\{(x_n,t_n)\\}_{n=1}^N$, where $x_n = \\mathbb{R}^d$, where $d \\geq 1$ represents the dimension of your data,  and $t_n \\in \\mathbb{R}$ which are the target values that each data $x_n$ corresponds to. \n",
        "\n",
        "Next, we seek a prediction function $y$ that takes in $x_n$ and outputs a prediction vector,\n",
        "$$\n",
        "  y(x) = \\textstyle\\sum\\limits_{i = 1}^d w_i x_i + b.\n",
        "$$  \n",
        "\n",
        "Let the squared loss be defined as,\n",
        "\\begin{equation}\n",
        "\t\\mathcal{L}(x,t) = \\dfrac{1}{2}(y(x) - t)^2, t \\in \\mathbb{R} \n",
        "\\end{equation}\n",
        "We can then the optimal prediction function by minimizing the mean squared error,\n",
        "\\begin{equation}\n",
        "\t\t\\mathcal{E}(w) = \\dfrac{1}{2N}\\sum\\limits_{n=1}^N \\mathcal{L}(x_n, t_n) = \\dfrac{1}{2N}\\sum\\limits_{n=1}^N (w^\\top x_n + b - t_n)^2 \\tag*{(mse)}\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNxXD6S6CXGb"
      },
      "source": [
        "In the following code, you will generate a data set $\\mathcal{D}$ consisting of $20$ 1D data points $x_n$ sampled from a uniform distribution along with targets $t_n$ assumed to be given by the formula,\n",
        "\n",
        "$$\n",
        "  t_n = \\sin(\\pi x_n) + 0.3*\\epsilon, \\epsilon \\sim \\mathcal{N}(0,1)\n",
        "$$\n",
        "Let us visualize these data point using the Matplotlib package. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lo79VxXU_hTe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "449e25ec-3918-4f73-fbfe-dba43adf1f20"
      },
      "source": [
        "n, d = 20, 1 #n represents data points and d represents the number of dimensions\n",
        "\n",
        "x = random.uniform(key, (n, d), dtype=np.float64,  minval = -5., maxval = 5.)    \n",
        "t = np.sin(np.pi*x) + 0.3*random.normal(key, (n, d))    \n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.scatter(x, t)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD4CAYAAADvsV2wAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAARx0lEQVR4nO3df6zd9V3H8dfLy9086vROexV6WyhG\n0ljtZuWkjmAiDmYLEtrVLcKigro0MSMOM2uoJC7ZP8U0cf4YERskA10GyyyXKsw7GDP4a8opt1AK\nu64SkJ6i3MEu03gibXn7xz233FvOOb2n3+893+/5fp6PpOn5/uB83jkkr++3n8/38/k6IgQAqL7v\nKLoAAMBgEPgAkAgCHwASQeADQCIIfABIxHlFF9DLqlWrYt26dUWXAQBD4+DBg9+MiPFOx0od+OvW\nrVOj0Si6DAAYGrZf7HaMLh0ASASBDwCJIPABIBEEPgAkgsAHgETkEvi277b9iu1nuhy/wvbrtg+1\n//xeHu0CAJYvr8cyPyvpM5Lu7XHO30fEtTm1BwCVMznd1N6pGR2fa2n1WE27tqzX9k0TuX1/LoEf\nEY/bXpfHdwFAiianm9q9/7BaJ05JkppzLe3ef1iScgv9QfbhX2b7Kdtfsv1j3U6yvdN2w3ZjdnZ2\ngOUBQHH2Ts2cDvsFrROntHdqJrc2BhX4T0q6KCLeK+lPJE12OzEi9kVEPSLq4+MdZwcDQOUcn2v1\ntf9cDCTwI+LbEfE/7c8PSxq1vWoQbQPAMFg9Vutr/7kYSODbPt+22583t9t9dRBtA8Aw2LVlvWqj\nI0v21UZHtGvL+tzayGXQ1vbnJV0haZXtY5I+KWlUkiLiTkkfkvQbtk9Kakm6PniZLgCctjAwu5JP\n6bjMuVuv14PVMgFg+WwfjIh6p2PMtAWARBD4AJAIAh8AEkHgA0AiCHwASASBDwCJIPABIBEEPgAk\ngsAHgEQQ+ACQCAIfABJB4ANAIgh8AEgEgQ8AiSDwASARBD4AJILAB4BEEPgAkAgCHwASQeADQCII\nfABIBIEPAIkg8AEgEQQ+ACQil8C3fbftV2w/0+W4bf+x7aO2n7b9k3m0CyANk9NNXX77Y7r41od0\n+e2PaXK6WXRJQymvO/zPStra4/jVki5p/9kp6U9zahdAxU1ON7V7/2E151oKSc25lnbvP0zon4Nc\nAj8iHpf0Wo9Ttkm6N+Z9TdKY7QvyaBtAte2dmlHrxKkl+1onTmnv1ExBFQ2vQfXhT0h6adH2sfa+\nt7G903bDdmN2dnYgxQEor+Nzrb72o7vSDdpGxL6IqEdEfXx8vOhyABRs9Vitr/3oblCB35S0dtH2\nmvY+AOhp15b1qo2OLNlXGx3Rri3rC6poeA0q8A9I+pX20zrvk/R6RLw8oLYBDLHtmya0Z8dGTYzV\nZEkTYzXt2bFR2zd17BVGD+fl8SW2Py/pCkmrbB+T9ElJo5IUEXdKeljSNZKOSvpfSb+aR7sA0rB9\n0wQBn4NcAj8ibjjL8ZD0sTzaAgCcm9IN2gIAVgaBDwCJIPABIBG59OEDeZqcbmrv1IyOz7W0eqym\nXVvWM2AH5IDAR6ksrJuyMJV+Yd0USYQ+kBFdOigV1k0BVg6Bj1Jh3RRg5RD4KBXWTQFWDoGPUmHd\nFGDlMGiLUlkYmOUpHSB/BD5Kh3VTgJVBlw4AJILAB4BE0KVTEswuBbDSCPwSYHYpgEGgS6cEmF0K\nYBAI/BLoNou0OdfS5DSv/gWQDwK/BHrNIt29/zChDyAXBH4JdJpduoCuHQB5YdC2BBYGZm+5/1DH\n4ywcBiAP3OGXxPZNE5pg4TAAK4jALxEWDgOwkujSKREWDgOwkgj8kmHhMAArJZcuHdtbbc/YPmr7\n1g7Hb7I9a/tQ+89H82gXALB8me/wbY9IukPSByQdk/SE7QMR8ewZp94fETdnbQ8AcG7yuMPfLOlo\nRDwfEW9Iuk/Sthy+FwCQozwCf0LSS4u2j7X3nekXbD9t+4u21+bQLgCgD4N6LPOvJa2LiPdIekTS\nPd1OtL3TdsN2Y3Z2dkDlAUD15RH4TUmL79jXtPedFhGvRsT/tTfvknRpty+LiH0RUY+I+vj4eA7l\nAQCkfAL/CUmX2L7Y9jskXS/pwOITbF+waPM6Sc/l0C4AoA+Zn9KJiJO2b5Y0JWlE0t0RccT2pyQ1\nIuKApN+0fZ2kk5Jek3RT1nYBAP1xRBRdQ1f1ej0ajUbRZQDA0LB9MCLqnY6xlg4AJILAB4BEEPgA\nkAgCHwASQeADQCIIfABIBOvhA8jN5HSTF/iUGIEPIBeT003t3n9YrROnJEnNuZZ27z8sSYR+SdCl\nAyAXe6dmTof9gtaJU9o7NVNQRTgTgQ8gF8fnWn3tx+DRpQMMubL0m68eq6nZIdxXj9UGXgs64w4f\nGGIL/ebNuZZCb/WbT043z/rf5m3XlvWqjY4s2VcbHdGuLesHXgs6I/CBIVamfvPtmya0Z8dGTYzV\nZEkTYzXt2bGRAdsSoUsHGGJl6zffvmmCgC8x7vCBIdatf5x+c3RC4ANDjH5z9IMuHWCILXSflOEp\nHZQfgQ8MOfrNsVwEPoBKKcu8hDIi8AFUBuv59MagLYDKKNO8hDIi8AFURtnmJZQNgQ+gMpiX0BuB\nD6AymJfQG4O2ACqDeQm95RL4trdK+iNJI5Luiojbzzj+Tkn3SrpU0quSfjEiXsijbQBYjHkJ3WXu\n0rE9IukOSVdL2iDpBtsbzjjt1yV9KyJ+RNKnJf1+1nYBAP3Jow9/s6SjEfF8RLwh6T5J2844Z5uk\ne9qfvyjpStvOoW0AwDLlEfgTkl5atH2sva/jORFxUtLrkn6g05fZ3mm7YbsxOzubQ3kAAKmET+lE\nxL6IqEdEfXx8vOhyAKAy8gj8pqS1i7bXtPd1PMf2eZK+T/ODtwCAAcnjKZ0nJF1i+2LNB/v1kj5y\nxjkHJN0o6Z8lfUjSYxERObSNEmCxKmA4ZA78iDhp+2ZJU5p/LPPuiDhi+1OSGhFxQNKfS/oL20cl\nvab5iwIqgMWqgOHhMt9o1+v1aDQaRZeBHi6//TE1O6xTMjFW0z/e+v4CKgLSZvtgRNQ7HSvdoC2G\nC4tVAcODwEcmLFYFDA8CH5mwWBUwPFg8DZmwWBUwPAh8ZMZiVcBwoEsHABJB4ANAIgh8AEgEgQ8A\niSDwASARBD4AJILAB4BEEPgAkAgCHwASQeADQCIIfABIBIEPAIkg8AEgEQQ+ACSCwAeARLAePipn\ncrrJC1mADgh8VMrkdFO79x9W68QpSVJzrqXd+w9LEqGP5NGlg0rZOzVzOuwXtE6c0t6pmYIqAsqD\nwEelHJ9r9bUfSEmmwLf9/bYfsf2N9t/v7nLeKduH2n8OZGkT6GX1WK2v/UBKst7h3yrpKxFxiaSv\ntLc7aUXET7T/XJexTaCrXVvWqzY6smRfbXREu7asL6gioDyyBv42Sfe0P98jaXvG7wMy2b5pQnt2\nbNTEWE2WNDFW054dGxmwBSQ5Is79P7bnImKs/dmSvrWwfcZ5JyUdknRS0u0RMdnjO3dK2ilJF154\n4aUvvvjiOdcHAKmxfTAi6p2OnfWxTNuPSjq/w6HbFm9ERNjudvW4KCKatn9Y0mO2D0fEv3c6MSL2\nSdonSfV6/dyvRkgez+MDS5018CPiqm7HbP+X7Qsi4mXbF0h6pct3NNt/P2/77yRtktQx8IE88Dw+\n8HZZ+/APSLqx/flGSQ+eeYLtd9t+Z/vzKkmXS3o2Y7tATzyPD7xd1sC/XdIHbH9D0lXtbdmu276r\nfc6PSmrYfkrSVzXfh0/gY0XxPD7wdpmWVoiIVyVd2WF/Q9JH25//SdLGLO0A/Vo9VlOzQ7jzPD5S\nxkxbVBLP4wNvx+JpqKSFgVme0gHeQuCjsrZvmiDggUXo0gGARBD4AJAIAh8AEkEffp+Yrg9gWBH4\nfWC6/nDiIg3MI/D70Gu6PgFSTnlcpLlgoCrow+8D0/WHT9Y1dRYuGM25lkJvXTAmp5srUC2wsgj8\nPvD6vOGT9SLNImyoEgK/D0zXHz5ZL9L8qw5VQuD3gdfnDZ+sF2n+VYcqYdC2T0zXHy5Z19TZtWX9\nkkFfiX/VYXgR+Ki8LBdpFmFDlRD4wFnwrzpUBX34AJAIAh8AEkHgA0AiCHwASASBDwCJIPABIBEE\nPgAkgsAHgERkCnzbH7Z9xPabtus9zttqe8b2Udu3ZmkTAHBust7hPyNph6THu51ge0TSHZKulrRB\n0g22N2RsFwDQp0xLK0TEc5Jku9dpmyUdjYjn2+feJ2mbpGeztA0A6M8g1tKZkPTSou1jkn6q28m2\nd0raKUkXXnjhyla2TLziDkAVnDXwbT8q6fwOh26LiAfzLigi9knaJ0n1ej3y/v5+8eJyAFVx1sCP\niKsyttGUtHbR9pr2vqHAi8sBVMUgHst8QtIlti+2/Q5J10s6MIB2c8Er7gBURdbHMj9o+5ikyyQ9\nZHuqvX+17YclKSJOSrpZ0pSk5yR9ISKOZCt7cHjFHYCqyPqUzgOSHuiw/7ikaxZtPyzp4SxtLVfe\nA6y84g5AVVTqjVcrMcBatVfc8cQRkK5KBf5KDbBW5RV3PHEEpK1Sa+kwwNpbrwsigOqr1B3+6rGa\nmh3CnQHWeb0uiHT1ANVXqTv8XVvWqzY6smQfA6xv6XbhG/uuUe3ef1jNuZZCb3X1TE4PzXQJAMtQ\nqcDfvmlCe3Zs1MRYTZY0MVbTnh0buVNt63ZBjBBdPUACKtWlI1VngHUldHvi6LfuP9TxfMY+gGqp\nXOCjt04XxL1TM4x9AAmoVJcOzg1jH0AauMNH5SaXAeiMwIckxj6AFNClAwCJqPQdPpOJAOAtlQ18\n1o0BgKUq26XDujEAsFRlA5+F1ABgqcoGPm+qAoClKhv4TCYCgKUqO2jLZCIAWKqygS8xmQgAFqts\nlw4AYCkCHwASUekunU6YfQsgVUkFPrNvAaQsU5eO7Q/bPmL7Tdv1Hue9YPuw7UO2G1nazILZtwBS\nlvUO/xlJOyT92TLO/dmI+GbG9jJh9i2AlGW6w4+I5yJiaG6PmX0LIGWDekonJH3Z9kHbO3udaHun\n7YbtxuzsbK5FMPsWQMrO2qVj+1FJ53c4dFtEPLjMdn46Ipq2f1DSI7a/HhGPdzoxIvZJ2idJ9Xo9\nlvn9y8LsWwApO2vgR8RVWRuJiGb771dsPyBps6SOgb/SmH0LIFUr3qVj+7ttv2vhs6Sf0/xgLwBg\ngLI+lvlB28ckXSbpIdtT7f2rbT/cPu2HJP2D7ack/aukhyLib7O0CwDoX6bHMiPiAUkPdNh/XNI1\n7c/PS3pvlnYAANmxlg4AJILAB4BEEPgAkAhH5Pqoe65sz0p6seg6lmGVpEKXjSgJfgd+A4nfQCr2\nN7goIsY7HSh14A8L242I6Lp4XCr4HfgNJH4Dqby/AV06AJAIAh8AEkHg52Nf0QWUBL8Dv4HEbyCV\n9DegDx8AEsEdPgAkgsAHgEQQ+Dmz/QnbYXtV0bUMmu29tr9u+2nbD9geK7qmQbG91faM7aO2by26\nnkGzvdb2V20/237P9ceLrqkotkdsT9v+m6JrOROBnyPbazW//PN/FF1LQR6R9OMR8R5J/yZpd8H1\nDITtEUl3SLpa0gZJN9jeUGxVA3dS0iciYoOk90n6WIK/wYKPS3qu6CI6IfDz9WlJv6P5VzomJyK+\nHBEn25tfk7SmyHoGaLOkoxHxfES8Iek+SdsKrmmgIuLliHiy/fm/NR94yb1pyPYaST8v6a6ia+mE\nwM+J7W2SmhHxVNG1lMSvSfpS0UUMyISklxZtH1OCYbfA9jpJmyT9S7GVFOIPNX/T92bRhXSSaT38\n1PR6v6+k39V8d06lLecdx7Zv0/w/8T83yNpQPNvfI+mvJN0SEd8uup5Bsn2tpFci4qDtK4qupxMC\nvw/d3u9re6OkiyU9ZVua78p40vbmiPjPAZa44s72jmPbN0m6VtKVkc4kj6aktYu217T3JcX2qObD\n/nMRsb/oegpwuaTrbF8j6Tslfa/tv4yIXyq4rtOYeLUCbL8gqR4RSa0YaHurpD+Q9DMRMVt0PYNi\n+zzND1Jfqfmgf0LSRyLiSKGFDZDn73TukfRaRNxSdD1Fa9/h/3ZEXFt0LYvRh488fUbSuyQ9YvuQ\n7TuLLmgQ2gPVN0ua0vxg5RdSCvu2yyX9sqT3t//fH2rf6aJEuMMHgERwhw8AiSDwASARBD4AJILA\nB4BEEPgAkAgCHwASQeADQCL+H+2E4xJuY+MbAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWZKrSwX_zEp"
      },
      "source": [
        "## 2.1 Directly Solving for $w^\\star$ Using the Normal Equation\n",
        "\n",
        "Since our data set is small, therefore we can directly find the optimal predictor using a bit of linear algebra.\n",
        "\n",
        "To account for the bias term, it is common to redefining our data and weight as $x_n := \\begin{bmatrix} 1 & x_n^\\top \\end{bmatrix}$ and $w := \\begin{bmatrix} b & w^\\top \\end{bmatrix}$, \n",
        "$$\n",
        "  y(x_n) = \\textstyle\\sum\\limits_{i = 1}^{d+1} w_i x_{n_i} = w^\\top x_n.\n",
        "$$\n",
        "\n",
        "Then, we can show that our mean squared error can be equivalently written as $$\\mathcal{E}(w) =    \\dfrac{1}{2N} \\|Xw-t\\|^2_2,$$\n",
        "where \\begin{equation}\n",
        "X = \\begin{bmatrix} x_1^\\top \\\\ \\vdots \\\\ x_N^\\top \\end{bmatrix}  \\in \\mathbb{R}^{N \\times (d+1)} \\quad \\text{and} \\quad t = \\begin{bmatrix} t_1\\\\ \\vdots \\\\ t_N \\end{bmatrix} \\in \\mathbb{R}^N\n",
        "\\end{equation}\n",
        "\n",
        "It can be shown that, $$w^\\star = (X^\\top X)^{-1}X^\\top t$$ minimizes the mean-squared error $\\mathcal{E}$. This expression is sometimes referred to as the *normal equation*. \n",
        "\n",
        "**In the space below, build the matrix $X$ and find the optimal solution $w^\\star$. Afterwards, plot the optimal prediction function against your dataset.**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKBI6I6FDewW"
      },
      "source": [
        "#Bult the X matrix here\n",
        "X = \n",
        "\n",
        "#Compute w using the normal equation\n",
        "w = \n",
        "\n",
        "#Write your optimal predictor here\n",
        "y = \n",
        "\n",
        "#Plot your prediction against data points\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJb5DlnU_MmR"
      },
      "source": [
        "## 2.2 Find $w^\\star$ Using Gradient Descent\n",
        "\n",
        "Next, you will use gradient descent to find the solution to the linear regression problem. Recall that the update equation for gradient descent (as studied in class) is given by,\n",
        "\n",
        "$$w_{k+1} = w_k - \\alpha \\nabla \\mathcal{E}(w_k)$$\n",
        "\n",
        "$\\alpha$ is our learning rate which is usually a small number, e.g., $0.1$\n",
        "\n",
        "**Implement a jax routine for calculating the gradient of $\\mathcal{E}$, and then run the gradient descent algorithm (say, for 100 iterations) until the optimal weight is found.** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdtO4WdyVyGW"
      },
      "source": [
        "### 2.2.1 **Define the mse function**\n",
        "\n",
        "Hint: your code could be similar to how we built the logistic function at the beginning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prvljG9lV8DI"
      },
      "source": [
        "# Build the mse function here\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MhXNfNGV995"
      },
      "source": [
        "### 2.2.2 **Implement Gradient Descent and Run for (say, 100) Interations**\n",
        "\n",
        "\n",
        "$$w_{k+1} = w_k - \\alpha \\nabla \\mathcal{E}(w_k)$$\n",
        "\n",
        "Hint: The function `grad` takes as arguments:\n",
        "-  `fun`: the numpy **function** for which the computation of the gradients is needed.\n",
        "- `argnums`: the **arguments** of the functions  with respect to which the function will be differentiated\n",
        "\n",
        "Example:\n",
        "```\n",
        "w_gradient, b_gradient = grad(fun = make_mse(x, t), argnums = (0,1))(w,b) \n",
        "```\n",
        "\n",
        "Then use calculated the gradient in your gradient descent update in a FOR loop\n",
        "."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvKaU7Oj9jtm"
      },
      "source": [
        "alpha = \n",
        "iterations = \n",
        "\n",
        "# Update w, b by calculating the gradient and making the update. \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cc9nsGzzWtIl"
      },
      "source": [
        "### 2.2.3 **Build your optimal linear prediction function and visualize your fit to the data**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_7U1E_YW-8K"
      },
      "source": [
        "# Your optimal linear prediction function here\n",
        "\n",
        "\n",
        "\n",
        "# Visualize your data and your fit\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLQ6TKZ2Xk8G"
      },
      "source": [
        "### 2.2.4 **(Optional) You can use jit to speed calculation.** \n",
        "\n",
        "**Repeat the gradient descent procedure as before but using jit to speed up the calculation of gradients.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cugsYg-zXv_0"
      },
      "source": [
        "# Build your jitted gradient descent iteration here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQJ1rmRrBdbT"
      },
      "source": [
        "#3.0 Overfitting and Regularization\n",
        "\n",
        "The goal of supervised machine learning is to make accurate predictions on data that are not from the training set, also known as the *test set*. However it is not guaranteed that the more data we train on, the better we will perform on the test set. \n",
        "\n",
        "It could happen that we fit our training set too well, so that a test data that does not look like the training data will cause a large error in our prediction. This is called **overfitting**: the scenario when a predictor that achieves a lower error on the training set results in a large error on the test set.\n",
        "\n",
        "To overcome overfitting, we can use a technique called regularization, which involves adding a function on to our mean squared error. In the following examples, we will step through several examples of regularization techniques. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpN2YCd6rZN-"
      },
      "source": [
        "## 3.1 Building a Dataset\n",
        "\n",
        "The code below creates a data set of  $n = 100$ examples generated from uniformly from $[-5, 5]$, each example is $5$ dimensional. The targets are assigned as follows, $$t  = x^\\top \\overline w + 0.1 * \\epsilon, \\epsilon \\sim \\mathcal{N}(0,1)$$ \n",
        "where $\\overline w$ is a randomly generated vector with normal distribution. \n",
        "\n",
        "\n",
        "**Your first task is to partition the data into 50 test examples, 20 validation examples and 30 test examples.** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3K6GbuJA6g2"
      },
      "source": [
        "d = 5                             #dimension of your data\n",
        "n = 100                           #total number of data points\n",
        "n_train = 50                      #total number of training examples\n",
        "n_validate = 20                   #total number of examples for validation \n",
        "n_test = 30                       #total number of examples for testing\n",
        "\n",
        "x_data = random.uniform(key, (n, d), dtype=np.float64,  minval = -5., maxval = 5.)    \n",
        "true_w = random.normal(key, (d,))\n",
        "t_data = x_data.dot(true_w) + 0.1 * random.normal(key, (n,))\n",
        "\n",
        "#Partition into training, validation and test set\n",
        "x_train = \n",
        "x_val = \n",
        "x_test = \n",
        "\n",
        "t_train = \n",
        "t_val = \n",
        "t_test = "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lo_PLbWzdCBX"
      },
      "source": [
        "## 3.2 Solve for $w^\\star$ Using Gradient Descent\n",
        "\n",
        "Next, using the same linear regresion code that you have built above, using gradient descent, compute the optimal predictor $y = x^\\top w^\\star + b^\\star$ corresponding to the mean-squared error \n",
        "$$\\mathcal{E}(w) = \\dfrac{1}{2N}\\sum\\limits_{n=1}^N (w^\\top x_n + b - t_n)^2$$ \n",
        "\n",
        "Afterwards, find the error between the predicted targets on the test set versus the true test targets using the formula,\n",
        "$$\\|y_\\text{test}- t_\\text{test}\\|_2$$\n",
        "where $y_\\text{test}$ is your prediction on the test set. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "239r52sHRgsK"
      },
      "source": [
        "#Define your mse function here \n",
        " \n",
        "\n",
        "#Perform gradient descent\n",
        "\n",
        "\n",
        "#Compute and print your test error\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dl1r0yIScAo7"
      },
      "source": [
        "## 3.3 $l_2$ regularization\n",
        "\n",
        "The first regularization technique we will introduce is called $l_2$ regularization. \n",
        "\n",
        "We define our mean-squared loss function as\n",
        "$$\\mathcal{E}_{l_2}(w) = \\dfrac{1}{2N}\\sum\\limits_{n=1}^N (w^\\top x_n + b - t_n)^2 + \\dfrac{\\lambda_2}{2}\\|w\\|^2_2$$\n",
        "where $\\lambda_2$ is a small positive constant we refer to as the regularization constant, e.g., $r = 0.01$. \n",
        "\n",
        "This optimization problem is called *ridge regression*. \n",
        "\n",
        "Make slight changes to your previous code to account for the $l_2$ norm, and using gradient descent, compute the optimal predictor $y = x^\\top w^\\star + b^\\star$ corresponding to this regularized mean-squared error."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-uUXTKk2WqK"
      },
      "source": [
        "#Define your mse function here \n",
        " \n",
        "\n",
        "#Perform gradient descent\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qu-GjdOuir0"
      },
      "source": [
        "### 3.3.1 Hyperparameter Tuning Using Validation Set\n",
        "\n",
        "Next, you are going to tune the regularization parameter $\\lambda_2$ using your validation set. \n",
        "\n",
        "Train several different models for $\\lambda_2 \\in \\{0.1, 0.01, 0.001\\}$. Then compute the error between your prediction on the validation set and the corresponding targets using the formula,\n",
        "$$\\|y_\\text{val}- t_\\text{val}\\|_2$$\n",
        "\n",
        "Pick the best performing model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5_AF5SjdJnK"
      },
      "source": [
        "#Your code Here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sydfmSVuvRU"
      },
      "source": [
        "### 3.3.2 Compute Test Error\n",
        "\n",
        "Finally, using the best performing model, find the error between the predicted targets on the test set versus the true test targets using the formula,\n",
        "$$\\|y_\\text{test}- t_\\text{test}\\|_2$$\n",
        "where $y_\\text{test}$ is your prediction on the test set. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q2rMuty9dIU2"
      },
      "source": [
        "# Your code Here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A18w7Gr9cJ0m"
      },
      "source": [
        "## 3.4 $l_1$ Regularization \n",
        "\n",
        "The next regularization technique we will introduce is called $l_1$ regularization. \n",
        "\n",
        "We define our mean-squared loss function as \n",
        "$$\\mathcal{E}_{l_1}(w) = \\dfrac{1}{2N}\\sum\\limits_{n=1}^N (w^\\top x_n + b - t_n)^2 + \\dfrac{\\lambda_1}{2}\\|w\\|_1$$\n",
        "where $\\lambda_1$ is again our regularization constant.\n",
        "\n",
        "This optimization problem is called the *LASSO*. \n",
        "\n",
        "**Repeat the previous experiment by training several different models and choosing the best one based on its performance on the validation set.** \n",
        "\n",
        "**Finally, pick the best performing model and use it to find the error between the predicted targets on the test set versus the true test targets.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncDETNSk_--d"
      },
      "source": [
        "# Your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jr1f2ax2cQ6V"
      },
      "source": [
        "## 3.5 Approximate $l_1$ Regularization\n",
        "\n",
        "A technicality is that the  $l_1$ norm is not differentiable, so JAX was actually avoiding the non-differentiable points.\n",
        "\n",
        "To address the non-differentiability of $l_1$ norm, Schmidt *et al.* [1] proposed a differentiable approximation to the $l_1$ norm.\n",
        "\n",
        "In this case, we define our mean-squared loss function as \n",
        "$$\\mathcal{E}_{\\widetilde{l_1}}(w) = \\dfrac{1}{2N}\\sum\\limits_{n=1}^N (w^\\top x_n + b - t_n)^2 + \\sum\\limits_{i = 1}^d \\dfrac{\\lambda}{a} (\\log(1+\\exp(a w_i)) + \\log(1-\\exp(-a w_i)))$$\n",
        "where both $a$ and $\\lambda$ are regularization constants.\n",
        "\n",
        "\n",
        "**Repeat the previous experiments by training several different models and choosing the best one based on its performance on the validation set.** \n",
        "\n",
        "(For convenience, you can fix $a$, say $a = 0.1$. Alternatively you can tune both $a$ and $\\lambda$)\n",
        "\n",
        "**Finally, pick the best performing model and use it to find the error between the predicted targets on the test set versus the true test targets.**\n",
        "\n",
        "[1] *Mark Schmidt, Glenn Fung, and Rmer Rosales. Fast\n",
        "optimization methods for l1 regularization: A comparative study and two new approaches. In European Conference on Machine Learning, pages 286–297.\n",
        "Springer, 2007.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNpXfqcJVhQd"
      },
      "source": [
        "# Your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyCNgrkzcW3r"
      },
      "source": [
        "## 3.6 Elastic Net\n",
        "\n",
        "Finally, we can define our mean-squared loss function as a combination of $l_1$ and $l_2$ norm, \n",
        "$$\\mathcal{E}_{l_1+l_2}(w) = \\dfrac{1}{2N}\\sum\\limits_{n=1}^N (w^\\top x_n + b - t_n)^2 + \\frac{r_1}{2}\\|w\\|_1 + \\dfrac{r_2}{2}\\|w\\|_2^2$$\n",
        "\n",
        "This regularizer is called the elastic net. \n",
        "\n",
        "**Repeat the previous experiments by training several different models and choosing the best one based on its performance on the validation set.** \n",
        "\n",
        "**Finally, pick the best performing model and use it to find the error between the predicted targets on the test set versus the true test targets.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZOjqRiPLtmr"
      },
      "source": [
        "# Your code here: \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hTRuzINYcwz"
      },
      "source": [
        "## 3.7 Report what you find\n",
        "\n",
        "How does the weight and bias compare under each regularization scheme? Which regularizer does the best on the test set?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4EfhAAht8qH"
      },
      "source": [
        "Report your finding here:"
      ]
    }
  ]
}